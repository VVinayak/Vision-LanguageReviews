Summary: The paper introduces the task of Visual Question Answering, with both the questions and answers being free form and open ended. Given an image and natural language question the task is to produce a corresponding natural language answer. VQA is more complex than generic image captioning in that there are underlying contexts that need to be captured by the answer based on the question being asked, which is open ended. However, the paper suggests that answers to such questions are of a few words or are one amongst a set of choices, and hence easier to evaluate automatically. They provide a dataset containing ∼0.25M images, ∼0.76M questions, and ∼10M answers. Numerous baselines and methods for VQA are provided and compared with human performance. 

