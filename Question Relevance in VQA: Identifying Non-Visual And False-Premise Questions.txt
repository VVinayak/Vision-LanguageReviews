Summary:
This paper proposes to check the relevance of questions asked about images in the context of Visual Question Answering. They first decide whether (1) a question is visual or not, (and if yes) then determine whether (2) the question is pertinent to the image under concern. Their approaches are based on VQA model uncertainty, LSTM-RNNs and measuring caption-question similarity. They conclude through human studies that a VQA model with question relevance reasoning abilities is perceived to be much smarter and human-like than normal VQA systems. They also outperform previous baselines on the tasks of (1) and (2).

Strengths:


Weaknesses:
- It remains unclear on how novel the methodology is, given that existing work has already done this with captions and queries for an image. It seems like an application of those ideas to the question domain instead of other text formats.

- A footnote mentions that the high accuracies on the above tasks indicated that the size of the data collected is sufficient to learn corresponding lnguistic structures.

- 

Reflections:
- Would it be more beneficial to train the LSTM network on scene graphs created from groundtruth captions/questions (as in SPICE) instead of just POS tags?

- It would have been interesting to see how relevant questions were from VQA2.0 assuming Visual Genome (or any other VQA dataset) questions to be perfectly relevant and vice versa 

Most Interesting thought:
