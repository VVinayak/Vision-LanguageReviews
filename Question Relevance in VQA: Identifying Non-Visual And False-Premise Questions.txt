Summary:
This paper proposes to check the relevance of questions asked about images in the context of Visual Question Answering. They first decide whether (1) a question is visual or not, (and if yes) then determine whether (2) the question is pertinent to the image under concern. Their approaches are based on VQA model uncertainty, LSTM-RNNs and measuring caption-question similarity. They conclude through human studies that a VQA model with question relevance reasoning abilities is perceived to be much smarter and human-like than normal VQA systems. They also outperform previous baselines on the tasks of (1) and (2).

Strengths:
- It is appreciated that they consider the per-class normalized accuracy and acknowledge the imbalance in the created datasets.

- Using the POS tags to train the LSTM seems quite intuitive to discriminate visual and non-visual questions. The approach to measure caption-question similarity is meaningful to measure visual grounding of questions, and is partially how the SPICE metric used in image captioning tasks is calculated.

- It is encouraging to note the overwhelming vote for the question reasoning model, inspite of having added just simple MLPs in front of similar VQA models.

Weaknesses:
- It remains unclear on how novel the methodology is, given that existing work has already done this with captions and queries for an image. It seems like an application of those ideas to the question domain instead of other text formats.

- When assuming that VQA models will answer questions with lesser certainty because of not having seen such training samples, it also needs to be remembered that current VQA models tend to memorize answers and fail on novel instances which might very well be relevant to the image.

- It would have been nice to see a larger set of questions being tested for human evaluations as meaningful or not. Its also not clear whether images were shown simultaneously with questions for the experiment. 

Reflections:
- The models seem to do much better at detecting false premises than true premises based on precision scores.

- It would have been interesting to see how relevant questions were from VQA2.0 assuming Visual Genome (or any other VQA dataset) questions to be perfectly relevant and vice versa. Then it would be interesting to see how many questions passed the test of being visual questiond but ended up having false premises.

- Generative models have a long way to go before catching up with discriminative models.

- The number of AMT workers involved in the experiment is very less and questionable. The idea that they may choose the paper's model over others even when it is wrong seems to show some unwanted statistical biases due to lack of data. 

Most Interesting thought:
It seemed as though the question-caption and question-generated question similarity scores seemed to better understand the semantics of an image. But when we, as part of our project, tried to measure the number of meaningful words present in a caption as compared to a question, we found it to be surprisingly low, i.e. there isn't much difference between captions and questions for images in VQA datasets. Its possible that the model gamed the system to produce such high accuracies using the similarity metrics.Even the SPICE metric seeks to learn a scene graph representation from the ground truth and generated captions and match their similarity. Though it might be a measure of visual grounding because workers gave the ground truth, we might need to start actually looking at image annotations and making decisions on the relevance of questions/making more complete captions. Datasets like Visual Genome and models like Fast R-CNN which do object detection before deciding on features to be extracted are grounds for future work for better VQA models. 
