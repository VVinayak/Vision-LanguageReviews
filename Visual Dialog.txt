Summary:
The paper introduces the task of free-form and open-ended Visual Dialog, where an AI agent is required to hold meaningful conversations and answer questions about visual content with humans by tapping into the dialog history, the image and a question about the image. A 2 person chat based dataset is collected (VisDial) with a total of 1.2 million dialog question-answer pairs  for 120,000 images from MS COCO. A baseline family of encoder-decoder models for solving visual dialog is provided (having 3 encoders - Late Fusion, Hierarchical Recurrent Encoder and Memory Network; and 2 decoders - generative and discriminative) that outperforms other sophisticated baselines. An evaluation protocol for visual dialog based on retrieval is also established, where an AI agent sorts through candidate answers and evaluates metrics such as mean-reciprocal-rank of human response. Human studies help measure how far the baselines are from human performance on the visual dialog task. A visual chatbot is also released. They finally assert that Visual Dialog is a good test for machine intelligence and allows sufficient evaluation of progress. 

Strengths:
- Visual Dialog is a good test on an AI's ability to not just see, but to also understand and communicate (region detection, co-reference resolution, visual memory, consistent answers). It is appreciated that this topic has been introduced to the community, and that much effort has been taken to build a cohesive dataset, which contains longer answers and is an order of magnitude larger. There is a sense of purpose for each actor (questioner doesn't see the image and tries to develop a mental model by asking questions; answerer sees the image and answers by contextualizing the current question in terms of the previous dialog).

- The task is a mix of goal-driven dialog (where time to task completion is important) and goal-free dialog (where AI agents must be more coherent and engaging) and is a better frontier for AI. There is also no visual priming bias as in VQA because the questioner doesn't see the image.

- Much care was taken to ensure that multi-user HITs were not interrupted and error handling protocols were covered well.

- Extensive statistics on the data are represented and satisfactorily explained.

- With longer and more unique answers, models can not piggyback on language priors as commonly observed in VQA. The answers and questions are not always contextually relevant due to the task, making for a richer and more human-like collection of data to help AI know when questions are contextually relevant to the image and answer/not answer on this basis. There is also continuity in questions and a need to understand coreference issues in questions for models to properly answer questions.

- The evaluation metric is intuitive - the best way to know how good the model is to understand its answer at every stage of the dialog by retrieval against human responses.

- Results are intuitive - image understanding is important for models and learning the history in improved models is better than naively just adding it to model inputs.

Weaknesses:
- The first 2 pages spend a great deal of space introducing the topic in general. This is good for a complete novice in vision-language tasks. But it has resulted in all important information including model design and results being pushed into the supplement which most reviewers are hesitant to read. Some sections were copied again into the Appendix (A1,2,3,5).

- It would have been interesting to see an evaluation of how many consecutive rounds of conversation each model successfullu answered. If it randomly oscillates between answering correctly and answering incorrectly, then we might have the language priors problem creeping into the visual dialog model as well.

Reflections:
- The dataset collected where the blurred pictures were shown to the questioner could be a good way to test image segmentation/detection algorithms or solve image inpainting tasks.

- Without the image, the VisDial questions were more open-ended but follow a pattern. This might mimic the actual knowledge discovery process of humans in unknown learning environments. It might be a better indicator of natural language, and help in knowledge base retrieval tasks which are visual in nature.  

- It is surprising to see the HRE models getting beaten by the MN models and are actually matched by Visual Dialog versions of VQA models. Is the method of encoding history crucial to help decide how well models perform?

- An adaptation of the Visual Turing test on this dataset would have been interesting. (Geman et al. PNAS 2014)

Most Interesting Thought:
The HRE encoder looks like a dialog version of Lu et al's (2016) visual sentinel model. However they didn't take visual features from regions but just the final vector. Using the region vectors might help improve performance. Maybe getting inspiration from more advanced captioning and VQA models such as bottom-up and top-down attention will definitely help improve the start of the art. 
