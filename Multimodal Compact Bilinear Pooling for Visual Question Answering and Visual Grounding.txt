Summary:
The paper introduces the concept of multimodal compact bilinear pooling to efficiently and expressively combine multimodal features from visual and textual representations for the tasks of visual question answering and visual grounding. They show that MCBP performs better than other techniques used in previous literature for combining both modalities, while being more efficient than an outer product of these two. They achieve state-of-the-art performance for visual question answering on the Visual7W and VQA datasets by using MCB twice - once for predicting attention over spatial features and again to combine the attended representation with the question representation. For the task of visual grounding, MCBP leads to improved phrase localization accuracy, indicating better interaction between query phrase representations and visual representations of proposal bounding boxes.
