Summary:
The paper introduces the concept of multimodal compact bilinear pooling to efficiently and expressively combine multimodal features from visual and textual representations for the tasks of visual question answering and visual grounding. They show that MCBP performs better than other techniques used in previous literature for combining both modalities, while being more efficient than an outer product of these two. They achieve state-of-the-art performance for visual question answering on the Visual7W and VQA datasets by using MCB twice - once for predicting attention over spatial features and again to combine the attended representation with the question representation. For the task of visual grounding, MCBP leads to improved phrase localization accuracy, indicating better interaction between query phrase representations and visual representations of proposal bounding boxes.

Strengths:
1. For combining the two vector representations (multimodal pooling), current approaches in VQA or grounding might not be expressive enough to fully capture the complex associations between the two different modalities. Bilinear pooling computes a multiplicative interaction between all elements of both vectors and are known to be beneficial for fine-grained classification for vision only tasks. To counter the problem of dimensionality, they extend Gao et al's (2016) idea to efficiently compress bilinear pooling for a single modality. MCB is approximated by randomly projecting the image and text representations to a higher dimensional space  and then convolving both vectors efficiently by using element-wise product in Fast Fourier Transform (FFT) space

2. The model does consider attention to capture spatial information. They also experiment with multiple attention maps to get the model to take multiple glimpses of important regions in the picture.

3. The evaluations are done in sufficient numbers and compared against a variety of baselines. The justification in terms of bilinear versus non-bilinear model performance despite having nearly similar parameter numbers is satisfactory.

Weaknesses:
1. The justification for using a MCB  is to use the numerous interactions between different modalities through the outer product. But the justification for using the outer product over joint multimodal embeddings isn't clear. How is the outer product capturing more information? Isn't it possible that they may be just introducing more noise and confuse the model? By multiplying random text components with random image components, it is possible that some noise is creeping in.

2. The testing has been done for question-answer pairs where the answers are mostly one word in length, and so can afford to have 3000 classes. It makes the testing incomplete and question the power of MCBP. 

3.There should have been more examples to get a qualitative evaluation of the results, including a few failures.

Reflections:
1. It would be interesting to know how MCB can compete with or complement current state of art models using joint embeddings.

2. Does having more number of parameters actually help our model? Or is it just more noise?

3. CountSketch vectors tend to mash up information from very high dimension vectors, which might cause loss of information. The choice of s(k) and h(k) is done randomly from a uniform distribution. Are the results a random coincidence, especially since the paper clearly mentions as s and h are maintained the same for future invocations of count sketch?

4. The authors augment their training data using the Visual Genome and Visual7W datasets. Is it right to compare the performance of models if trained on different amounts of training data?

5. The reason for using MCBP is that it captures more features and does so in an efficient way by a specialized mapping-summation. It would have been good to know some comparisons on computation time/space complexity between previous element wise techniques and MCBP with FFT computations.

Most interesting thought:
It would be interesting to see how MCBP could work on generic image captioning tasks, when given the image and caption in late fusion. It would also be interesting to extend the capabilities of the above model to be able to give open ended answers instead of having to pick from binned options. We could use a similar approach as in captioning/visual grounding, which would pass (through an LSTM) the image region vectors weighted by the distribution of answers relevant to that region based on intersection of union over bounding boxes. Work by Lin and Parikh (2016) could also help in extending this idea.
