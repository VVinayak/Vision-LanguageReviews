Summary:
Learning visual classifiers from weakly labeled image captioning data is a hard problem which involves finding a correspondence between the nouns and image regions. To improve visual classifiers for object recognition using weakly labelled data, the authors consider prepositions and adjectives, and not just nouns, to constrain the determination of the correspondence between image regions and semantic object classes. This helps learn object relationships which help solve the correspondence ambiguities. They simultaneously learn the visual features defining “nouns” and the differential visual features defining such “binary-relationships” using an EM-based approach. They also present a more constrained Bayesian model with priors over relationships between nouns for the labeling process. Experimental results show that using relationship words helps in reduction of correspondence ambiguity and using a constrained model leads to a better labeling performance.

Strengths:
- The paper attemts to spatially localize detected objects in an image while resolving co-occurrence relationships that may be tough to resolve, by utilizing relationships between objects in natural images in the form of prepositions and comparative adjectives. 

- In an era before deep learning, they have intelligently realized that binary relationship vocabulary is much smaller than that of nouns, and have hence been able to create rule based classifiers for such relationships. These classifiers are based on differential features extracted from pairs of regions in an image.

- Simultaneous learning of nouns and relationships reduces correspondence ambiguity and leads to better learning performance, which is quite intuitive. Many previous papers were unable to solve the correspondence ambiguity through their Bag of Words approach.

- Learning priors on relationships that exist between nouns constrains the annotation problem and leads to better labeling and localization performance on the test dataset as it reduces the search space.

Weaknesses:
- In the era of deep learning, such hand codings are not very efficient and would not generalize well to all types of labelled image data. We just need a bag of vocabulary to create embeddings for neural networks to learn their language from.

-

Reflections:
-  In the related work, the authors mention that the idea of creating a training dataset of all visual attributes is cumbersome. We must be extremely grateful to Dr. Fei Fei Li and Amazon Mechanical Turk for providing the platform to do deep learning research in computer vision.


Most Interesting Thought:

