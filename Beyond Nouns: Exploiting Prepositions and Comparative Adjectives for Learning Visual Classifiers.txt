Summary:
Learning visual classifiers from weakly labeled image captioning data is a hard problem which involves finding a correspondence between the nouns and image regions. To improve visual classifiers for object recognition using weakly labelled data, the authors consider prepositions and adjectives, and not just nouns, to constrain the determination of the correspondence between image regions and semantic object classes. This helps learn object relationships which help solve the correspondence ambiguities. They simultaneously learn the visual features defining “nouns” and the differential visual features defining such “binary-relationships” using an EM-based approach. They also present a more constrained Bayesian model with priors over relationships between nouns for the labeling process. Experimental results show that using relationship words helps in reduction of correspondence ambiguity and using a constrained model leads to a better labeling performance.

Strengths:
- The paper attemts to spatially localize detected objects in an image while resolving co-occurrence relationships that may be tough to resolve, by utilizing relationships between objects in natural images in the form of prepositions and comparative adjectives. 

- In an era before deep learning, they have intelligently realized that binary relationship vocabulary is much smaller than that of nouns, and have hence been able to create rule based classifiers for such relationships. These classifiers are based on differential features extracted from pairs of regions in an image.

- Simultaneous learning of nouns and relationships reduces correspondence ambiguity and leads to better learning performance, which is quite intuitive. Many previous papers were unable to solve the correspondence ambiguity through their Bag of Words approach.

- Learning priors on relationships that exist between nouns constrains the annotation problem and leads to better labeling and localization performance on the test dataset as it reduces the search space and uses second order contextual data to help resolve ambiguities.

- Its' quite fascinating to see an early object detection system that did a decent job at annotating images. This form of hand coded attention through the picking of adjectives and prepositions is a pre-cursor to Visual Sentinel and HieCoAtt in giving attention to the right words and learning classifiers accordingly. 

Weaknesses:
- In the era of deep learning, such hand codings are not very efficient and would not generalize well to all types of labelled image data. We just need a bag of vocabulary to create embeddings for neural networks to learn their language from.

- It is not immediately clear as to what rules were used to compute differential fatures between pairs of objects.

- We see that the test set had to be manually chosen to have vocabulary that it was trained on. The model could not use the original dataset's annotations as well because of the mismatch in vocabulary sizes. This is problematic and questions the ability of a model to generalize to novel situations, as well be tested appropriately. Hence human evaluations were required to check for correctness. 

Reflections:
-  In the related work, the authors mention that the idea of creating a training dataset of all visual attributes is cumbersome. We must be extremely grateful to Dr. Fei Fei Li and Amazon Mechanical Turk for providing the platform to do deep learning research in computer vision and collect huge datasets required for such research.

- The paper states that a one-one relationship between semantic object classes and the nouns in the annotation is assumed. This problem is existent even in today's datasets.

- It is so astonishing to see how pre-deep learning frameworks were forced to make do with small datasets for words and labelled images, and still expected to be performing state-of-art.

Most Interesting Thought:
There was excessive handcoding involved in the preparation of the training and testing datasets, both in terms of images and vocabulary. The deployment of deep neural networks and massive datasets have eliminated the need for such handcoding. The papaer represents an early view into how object recognition was done. It would be interesting to see how an EM framework could be incorporated into existing models to help improve accuracies of models in VQA and image captioning. It would also be interesting to use such algorithms in generating scene graphs of images to better help caption such images, or answer questions about these images.
