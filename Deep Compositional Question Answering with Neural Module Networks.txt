Summary:
The paper introduces the concept of neural module networks, which compose collections of jointly-trained neural “modules” into deep networks for question answering. Their approach decomposes natural language questions (which are generally compositional in nature) into linguistic sub-structures that help in the dynamic instantiation of modular networks consisting of reusable components. The final compound networks are jointly trained. The model achieves state-of-art performance on the VQA dataset (especially on compositional questions), and the SHAPES dataset (which was a synthetic dataset of abstract shapes created by the authors to test compositional understanding of VQA models).

Strengths:
- It is quite intuitive that current VQA models are monolithic networks being expected to understand the compositional nature of natural language questions without being trained to understand them. The idea of training the model through separate reusable modules to better understand this compositionality and visual features is quite interesting, as compared to previous works that purely rely on fixed logical inference.

- The modular nature of sub-structures means that modules can be composed in an independent manner, can be done so in a different way for every question, can be reused and possibly without any having seen such structures during training. The specialization required of an assembled network arises naturally from the training objective.

- The paper introduces the SHAPES dataset to enable study into improving compositional nature of questions for complex images. It also performs well on the VQA dataset for compositional questions.

- The general architecture for Neural Module Networks is versatile in the sense that it can be used in multiple other applications such as text question answering, visual referring expression resolution, querying structured knowledge bases or even general signal processing.

- A "Visual SQL" of sorts is developed where an outside user can pass modular queries to the model once completely trained.

- The model combines the output of the NMN with predictions from a simple LSTM question encoder to prevent a change in grammatical semantics during modularization to affect the answer, and to help capture semantic regularities.

- The paper takes care of the situation where some weights are updated much more frequently than others while training the joint model.

- On the SHAPES dataset, the model is able to generalize to much more complicated questions than seen in the training set.

Weaknesses:
- The modules seem hand-coded and limited. It is also not apparent what intuition is being used in devising a module's structure. The potential position of a module seems to be fixed beforehand, hence preventing any flexibility in design of the question structure.

- The SHAPES dataset has yes/no as answers to all questions, making it easy for the model to memorize answers based on priors. There is not much intuition/explanation on how this problem is averted. 

- The measure module's inappropriate hand-coded structure appears to have caused overfitting while answering yes/no questions in the VQA dataset thus reducing its' accuracy compared to other sequence baselines.

- A better parser is necessary especially for complicated and long questions, which is the primary target space for NMNs as they try to solve the problem of compositionality.

- The model does get confused semantically sometimes and provide answers that are learnt from dataset priors but are not related to the image.

Reflections:
- The paper has found a newer technique to use attention (through compositional attention blocks), which is more hand coded instead of leaving it to a monolithic network. AI is still too young to be left alone without human supervision.

- The paper trains modules jointly and helps create their specific role because of training over specific image-answer pairs and NMN parameters. This modular extraction can be used to improve image captioning models through such module-based attention. 

- The paper clearly highlights the need for datasets which require more complex reasoning from models, as current datasets contain very simple questions, which are not of much use if used to train models for future applications of VQA. Existing datasets don't test models for robustness in the presence of noise.

- There seem to be few questions and more images in the SHAPES dataset. Isn't it possible for the model to just memorize the questions and their answers, which is not intended?

Most Interesting thought:
It would be interesting to study how the integration of a knowledge graph could help create the structure of each module, and possibly teach the model how to focus attention over the question to use it in the creation of the NMN architecture. The modules' structure currently seems to be handcoded. It would also be interesting to use NMNs to help in discriminative image captioning through a discriminate module (find + subtract/AND) while giving context agnostic data.
