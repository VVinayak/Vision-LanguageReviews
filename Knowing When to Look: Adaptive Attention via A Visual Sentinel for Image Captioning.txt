Summary:
The paper builds a visual sentinel along with an adaptive attention model for image captioning. At each time step, the model decides whether to attend to the image (and to which regions) or the visual sentinel, in order to help it in sequential generation of words (so attending to the non-visual words is eliminated). State-of-the-art results are achieved on MS-COCO and Flickr30K datasets by a significant margin.

Strengths:


Weaknesses:


Reflections:


Most interesting thought:
