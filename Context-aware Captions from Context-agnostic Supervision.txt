Summary:
The paper develops inference techniques that describe images in a discriminative context-aware way despite being trained on context-agnostic training data, where the model is able to accurately describe a target image differentially from a distractor image. A joint inference is learnt from a language model that is context-agnostic and a listener which distinguishes closely-related concepts. The paper tests the technique on a justification task that aims to explain why an image belongs to a particular class as compared to another relevant class from the CUB-200-2011 dataset. The other task of discriminative image captioning to uniquely describe a target image as compared to a distractor image from the COCO dataset. Evaluations of results shows state-of-art performance on generative and listener-speaker approaches for discrimination.

Strengths:
1. Introspective listener able to develop pragmatic reasoning skills despite being trained in a context-agnostic way

2. Unified inference of language model and listener helps in efficient search for discriminative sentences. Reusing the sampled distribution from the generative model for the listener model's discriminativeness training is useful. This allows the extension of context-agnostic models to context-aware models without additional training. 

3. The paper creates a new dataset specifically for justification tasks (CUB-Justify)

4. Andreas and Klein (2016) use 2 separate models for each task and also train on the abstract scenes dataset, whereas this paper trains on real world images from CUB-200-2011 and COCO.

5. The paper performs beam search on a modified objective for the introspective speaker model to induce discrimination

6. Conditioning the model on the image and the target concept helps improve the discriminative nature of the generator. The generator utters a sentence which is discriminately scored by a listener, which checks the sentence against the target and distractor concepts. The reasoning speaker tries to choose the best sentences while taking into account the listener's scores. 

7. The listener does not have to be trained separately - it is trained on an existing language model used by the speaker.  

8. The speaker-listener model helps in better discrimination of highly similar concepts and makes it more unified and efficient in terms of inference, due to the log-likelihood approximation of the emitter (conditioned on the target concept) and the suppressor (conditioned on the distractor concept). Thus the paper emits words similar to the target but rejects tokens similar to the distractor.

9. 

Weaknesses:
1. The listener module is dependent on the generator module to decide its discriminative power. Though this alleviates problems with training another module, the listener is not independently discriminative

2. 

Reflections:
1. Only concepts from the target and distractor are passed to the model, not the distractor image. 

2. Without the discriminator concepts, captioning can not improve. But it is usually tough to find distractor and target concepts in real-world applications.

3. If concepts are similar, then why are we trying to eliminate words that describe the distractor? Shouldn't we aim to add extra tokens to explain the difference between the target and the distractor, instead of removing the commonalities?

Most interesting Thought:
